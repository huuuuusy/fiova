
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="FIOVA (Five-in-One Video Annotations) Benchmark">
  <meta name="keywords" content="Video Caption, Video Understanding, LVLM Evaluation, Human-machine Comparison">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FIOVA (Five-in-One Video Annotations) Benchmark</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<meta property="og:title" content="FIOVA (Five-in-One Video Annotations) Benchmark" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://huuuuusy.github.io/fiova/" />
<meta property="og:url" content="https://huuuuusy.github.io/fiova/" />
<meta property="og:site_name" content="FIOVA (Five-in-One Video Annotations) Benchmark" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://huuuuusy.github.io/fiova/static/images/logo.png" />
<!-- <meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="FIOVA (Five-in-One Video Annotations) Benchmark" />
<meta name="twitter:description" content="FIOVA (Five-in-One Video Annotations) Benchmark." />
<meta name="twitter:site" content="@CanyuChen3" />
<meta name="twitter:image" content="https://huuuuusy.github.io/fiova/static/images/logo.png" /> -->
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"FIOVA (Five-in-One Video Annotations) Benchmark","name":"FIOVA (Five-in-One Video Annotations) Benchmark","url":"https://huuuuusy.github.io/fiova/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .grey-box {
        background-color: #c0c0c0; /* Grey color */
        color: rgb(70, 70, 70); /* White text color */
        padding: 20px; /* Padding inside the box */
        margin: 20px; /* Margin outside the box */
        text-align: center; /* Center the text */
    }
  </style>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">


</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/logo_1.png" alt="FIOVA (Five-in-One Video Annotations) Benchmark" width="900"/>
            <h1 class="title is-4 publication-title">This initiative aims to evaluate the gap between human and machine video understanding by benchmarking LVLMs against comprehensive human annotations.
            </h1>
            <h1 class="title is-5 publication-title">(Contact: <a href="https://huuuuusy.github.io/" target="_blank">Shiyu Hu</a>)
            </h1>

            <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
            <div class="content has-text-justified">
            <h3 class="head-h2" style="margin-bottom: 0.5em;color: rgb(255, 93, 93)">
              Latest News
            </h3>
                <!-- <li>
                    [2024.10.22] We have updated the <a href="https://arxiv.org/abs/2410.15270" target="_blank">arXiv version</a> of the paper.
                </li> -->
                <li>
                    [2025.05.15] The home page has been released! 
                </li>
            </div>
            </div>
            </div>
            </ul>
          </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="Can-LVLMs-Describe-Videos-Like-Humans" class="title is-2 publication-title">FIOVA: A Multi-Annotator Benchmark for Human-Aligned Video Captioning
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://huuuuusy.github.io/">Shiyu Hu*</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/Xuchen-Li">Xuchen Li*</a><sup>2,3,4</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/XuzhaoLi">Xuzhao Li</a><sup>5</sup>,
              </span>
              <span class="author-block">
                Jing Zhang<sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=nMe_kLAAAAAJ">Yipei Wang</a><sup>6</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.xinzhaoai.com/">Xin Zhao✉️</a><sup>7</sup>,
              </span>
              <span class="author-block">
                <a href="https://dr.ntu.edu.sg/cris/rp/rp02319">Kang Hao Cheong✉️</a><sup>1,8</sup>
              </span>
            </div>
            
            <div class="is-size-6 publication-authors">
              <sup>1</sup>School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore<br/>
              <sup>2</sup>Institute of Automation, Chinese Academy of Sciences, China<br/>
              <sup>3</sup>Zhongguancun Academy, Beijing, China<br/>
              <sup>4</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences, China<br/>
              <sup>5</sup>School of Automation, Beijing Institute of Technology, China<br/>
              <sup>6</sup>Institute of Automation, Southeast University, China<br/>
              <sup>7</sup>School of Computer and Communication Engineering, University of Science and Technology Beijing, China<br/>
              <sup>8</sup>College of Computing and Data Science, Nanyang Technological University, Singapore
            </div>
            
            
            <div class="column has-text-centered">
              <div class="publication-links">

                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.15270" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/huuuuusy/FIOVA"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-archive"></i>
                    </span>
                    <span>Dataset</span>
                    </a>
                </span>


              </div>
            </div>
            <!-- <div class="is-size-5 publication-authors">
              Published at <b><i>Proceedings of ICLR 2024</i></b>
            </div> -->
          </div>
        </div>
      </div>
    </div>


  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
              <figure>
                <img src="static/images/f1.png" style="width:100%" alt="Figure 0">
              </figure>
          </div>
        </div>
      </div> 
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite rapid progress in large vision-language models (LVLMs), existing video caption benchmarks remain limited in evaluating their alignment with human understanding. Most rely on a single annotation per video and lexical similarity-based metrics, failing to capture the variability in human perception and the cognitive importance of events. These limitations hinder accurate diagnosis of model capabilities in producing coherent, complete, and human-aligned descriptions.
              To address this, we introduce <strong>FIOVA</strong> (<strong>F</strong>ive-<strong>I</strong>n-<strong>O</strong>ne <strong>V</strong>ideo <strong>A</strong>nnotations), a human-centric benchmark tailored for evaluation. It comprises 3,002 real-world videos (~33.6s each), each annotated independently by five annotators. This design enables modeling of semantic diversity and inter-subjective agreement, offering a richer foundation for measuring human–machine alignment.
              We further propose <strong>FIOVA-DQ</strong>, an event-level evaluation metric that incorporates cognitive weights derived from annotator consensus, providing fine-grained assessment of event relevance and semantic coverage. Leveraging FIOVA, we conduct a comprehensive evaluation of nine representative LVLMs and introduce a complexity-aware analysis framework based on inter-annotator variation (CV). This reveals consistency gaps across difficulty levels and identifies structural issues such as event under-description and template convergence.
              Our results highlight FIOVA’s diagnostic value for understanding LVLM behavior under varying complexity, setting a new standard for cognitively aligned evaluation in long-video captioning. The benchmark, annotations, metric, and model outputs are publicly released to support future evaluation-driven research in video understanding.
            </p>
            
            </p>
          </div>
        </div>
      </div>
    </div>
  <!-- </section>
  <section class="section"> -->
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <!-- <br /> -->
      <div style="text-align:center">
        <h2 class="title is-3">Our Contributions</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>(1) <b>Human-centric dataset with multi-annotator diversity:</b> We introduce FIOVA, a benchmark comprising 3,002 real-world videos (average 33.6 seconds) covering 38 themes with complex spatiotemporal dynamics. Each video is annotated by five independent annotators, capturing diverse perspectives and establishing a cognitively grounded baseline for human-level video description. This design enables modeling of semantic variability and inter-subjective agreement, offering an authentic testbed for evaluating long video comprehension.</p>

        <p>(2) <b>Comprehensive evaluation of nine representative LVLMs:</b> We evaluate nine leading LVLMs, including GPT-4o, InternVL-2.5, and Qwen2.5-VL, across both full and high-complexity (<b>FIOVA<sub>hard</sub></b>) subsets. Our tri-layer evaluation framework integrates traditional lexical metrics, event-level semantic alignment (AutoDQ), and cognitively weighted evaluation (FIOVA-DQ), revealing trade-offs between precision and recall as well as consistent model weaknesses under narrative ambiguity.</p>
        
        <p>(3) <b>Complexity-aware human–machine comparison with cognitive alignment:</b> We propose a novel analysis framework based on inter-annotator variation (CV) to categorize videos by difficulty, enabling batch-level diagnosis of model robustness and behavior patterns. Our findings show that while humans increase descriptive diversity under complexity, LVLMs often converge to rigid, template-like outputs. FIOVA-DQ effectively captures this divergence by emphasizing event importance and aligning better with human judgment.</p>
        

        <div style="text-align:center">
          <h2 class="title is-3">Construction of the FIOVA Dataset</h2>
        </div>
        <div class="content has-text-justified">
          <br>
          <p>
            We introduce <b>FIOVA (Five-In-One Video Annotations)</b>, a benchmark specifically constructed to systematically evaluate the semantic and cognitive alignment of large vision-language models (LVLMs) in long-video comprehension tasks. The dataset comprises <b>3,002 real-world videos</b> (average duration: 33.6 seconds), covering <b>38 thematic categories</b>. Each video was <b>independently annotated by five human annotators</b> based solely on the visual content, excluding audio or subtitles. This multi-annotator design supports modeling of semantic diversity and inter-subjective agreement, providing a solid foundation for human-aligned evaluation.
          </p>
        
          <div class="columns is-centered">
            <figure>
              <img src="static/images/t1.png" alt="Table 1: Comparison of datasets">
            </figure>
          </div>
        
          <p>
            Compared to existing datasets, FIOVA offers substantial advantages in annotation richness and detail: each video is described using <b>paragraph-level captions averaging 63.3 words</b>, which is <b>4–15× longer</b> than captions in mainstream benchmarks (see <b>Table 1</b>). To construct a unified reference, we <b>fuse the five independent annotations</b> into a single <b>groundtruth caption</b> using a GPT-based synthesis process. This groundtruth preserves key events, contextual details, and temporal order, enabling robust model evaluation (see <b>Figure 4</b>).
          </p>
        
          <div class="columns is-centered">
            <figure>
              <img src="static/images/f4.png" alt="Figure 4">
              <!-- <figcaption style="text-align:center"><i>Figure 4. Example of multi-annotator captions, GPT-synthesized groundtruth, and FIOVA-DQ computation.</i></figcaption> -->
            </figure>
          </div>
        
          <p>
            To assess annotation consistency, we score the five human captions for each video along five semantic dimensions: <b>consistency, context, correctness, detail orientation, and temporality</b>. We then compute the <b>coefficient of variation (CV)</b> across annotators for each video and use it to group the dataset into <b>eight complexity levels</b> (Groups A–H). Group A contains videos with the highest annotator agreement, while Group H includes highly ambiguous or subjective cases with substantial annotation disagreement.
          </p>
        
          <p>
            Based on this grouping, we define a challenge subset <b>FIOVA<sub>hard</sub></b>, which consists of Groups F–H. These videos are characterized by frequent scene transitions, multi-perspective narratives, and low human agreement, making them ideal for stress-testing LVLMs under cognitively demanding conditions.
          </p>
        </div>
        

        <div style="text-align:center">
          <h2 class="title is-3">LVLMs Response Collection</h2>
        </div>
        <div class="content has-text-justified">
          <br>
          <p>
            To construct the FIOVA benchmark, we collected video captions generated by six state-of-the-art large vision-language models (LVLMs): <b>VideoLLaMA2</b>, <b>LLaVA-NEXT-Video</b>, <b>Video-LLaVA</b>, <b>VideoChat2</b>, <b>Tarsier</b>, and <b>ShareGPT4Video</b>. Each model was prompted with the same task: generate a detailed caption describing the visual content of <b>3,002 videos</b> from the FIOVA dataset.
          </p>
          <p>
            For consistency and fairness, we applied unified prompting strategies and carefully tuned generation parameters for each model, including <i>temperature</i>, <i>max token length</i>, and <i>sampling method</i>. All models were run on 8-frame video inputs. In total, <b>18,012 model responses</b> were collected, forming a large-scale corpus of video-description-response triplets. This collection enables structured evaluation of video understanding capabilities and direct comparison with human-annotated groundtruths.
          </p>
          <p>
            To complement our main evaluation, we further perform stress testing on <b>FIOVA<sub>hard</sub></b>—a subset comprising the most semantically complex videos with low annotator agreement (Groups F–H). Beyond the six baseline models, we additionally include three frontier LVLMs—<b>InternVL-2.5</b>, <b>Qwen2.5-VL</b>, and <b>GPT-4o</b>—exclusively on this subset. This extended setting enables a focused assessment of model robustness and alignment under cognitively challenging conditions.
          </p>          
          <p>
            Combined with our cognitively weighted event-level evaluation framework FIOVA-DQ (see <b>Figure 4</b>), this comprehensive model response collection provides a foundation for in-depth analysis of model behaviors, failure modes, and human alignment across both typical and challenging video scenarios.
          </p>
        </div>
        
        
        <div style="text-align:center">
          <h2 class="title is-3">Overall Evaluation for LVLMs</h2>
        </div>
        <div class="content has-text-justified">
          <br>
          <p>
            We evaluate the six baseline LVLMs using three complementary families of metrics: <b>traditional lexical metrics</b> (BLEU, METEOR, GLEU), the <b>event-based AutoDQ</b> framework, and our proposed <b>cognitively weighted FIOVA-DQ</b>. Each metric captures different aspects of model output quality—surface fluency, structural completeness, and human-aligned semantic relevance, respectively.
          </p>
          <p>
            As shown in <b>Table 3</b>, <b>Tarsier</b> achieves the highest F1 and recall under both AutoDQ and FIOVA-DQ, indicating strong event coverage. However, this comes at the cost of lower precision due to frequent overgeneration. In contrast, <b>ShareGPT4Video</b> attains the highest precision across both metrics but suffers from the lowest recall, revealing a conservative captioning strategy that often omits important content.
          </p>
          <p>
            Models such as <b>VideoLLaMA2</b> and <b>LLaVA-NEXT-Video</b> fall between these two extremes, exhibiting moderate trade-offs. Importantly, our evaluation confirms that <b>FIOVA-DQ achieves the strongest correlation with human preference</b> (Spearman ρ = 0.579), validating its effectiveness as a cognitively grounded metric.
          </p>
          <p>
            These results highlight the limitations of relying solely on traditional lexical overlap and emphasize the value of semantically and cognitively informed evaluation frameworks for diagnosing model behaviors in complex video captioning tasks.
          </p>
        </div>
        <div class="columns is-centered">
          <figure>
            <img src="static/images/t3.png" alt="Table 3">
            <!-- <figcaption style="text-align:center"><i>Table 3. Overall evaluation of six LVLMs under BLEU, AutoDQ, and FIOVA-DQ.</i></figcaption> -->
          </figure>
        </div>
        <br>
        
        <div style="text-align:center">
          <h2 class="title is-3">Batch Score Evaluation for LVLMs</h2>
        </div>
        <div class="content has-text-justified">
          <br>
          <p>
            To examine model robustness under different levels of semantic complexity, we divide the FIOVA dataset into <b>eight sub-groups (Groups A–H)</b> based on the coefficient of variation (CV) across six annotation dimensions. These groups reflect varying degrees of annotator agreement and thus serve as a proxy for input ambiguity and difficulty.
          </p>
          <p>
            We evaluate each LVLM’s performance within these groups using both <b>AutoDQ</b> and <b>FIOVA-DQ</b> metrics. As shown in <b>Figure 7</b>, <b>Tarsier</b> achieves the highest recall and F1 scores across most groups, demonstrating strong temporal modeling. In contrast, <b>ShareGPT4Video</b> consistently yields the highest precision but at the cost of lower recall, reflecting a conservative captioning strategy. All models exhibit clear performance drops in <b>Group H</b>—which contains the most cognitively complex videos—revealing that current LVLMs still struggle with multi-event, ambiguous scenarios.
          </p>
          <p>
            These results reveal distinct behavioral strategies: models like Tarsier tend to prioritize recall and semantic coverage, while others like ShareGPT4Video favor precision and stylistic consistency. This trade-off highlights the importance of building <b>balanced LVLMs</b> that can adapt to both simple and complex video inputs.
          </p>
          
        
        <div class="columns is-centered">
          <figure>
            <img src="static/images/f7.png" alt="Figure 7">
            <!-- <figcaption style="text-align:center"><i>Figure 7. Radar plots of LVLM performance across CV-based sub-groups under BLEU, AutoDQ, and FIOVA-DQ metrics.</i></figcaption> -->
          </figure>
        </div>

        <p>
          In <b>Figure 8</b>, we further analyze consistency trends by comparing CV rankings between humans and models. Interestingly, models show higher variability in simple videos (Groups A–B), whereas human annotations become more diverse in complex videos (Groups F–H). This inverse trend suggests that LVLMs resort to safer, template-like generations under uncertainty, while humans diversify their interpretations—underscoring the need for cognitively aligned evaluation frameworks.
        </p>
      </div>
      
        <div class="columns is-centered">
          <figure>
            <img src="static/images/f8.png" alt="Figure 8">
            <!-- <figcaption style="text-align:center"><i>Figure 8. Comparison of CV rankings between humans and LVLMs across groups (a) and ranking differences (b).</i></figcaption> -->
          </figure>
        </div>
        <br>
        
        <div style="text-align:center">
          <h2 class="title is-3">Evaluation of 9 LVLMs on FIOVA<sub>hard</sub></h2>
        </div>
        <div class="content has-text-justified">
          <br>
          <p>
            We evaluate nine representative LVLMs on <b>FIOVA<sub>hard</sub></b>—a high-complexity subset including Groups F–H—to diagnose model performance under cognitively demanding conditions. As shown in <b>Table 5</b>, <b>Tarsier</b> achieves the highest recall (0.636) in FIOVA-DQ, reflecting strong event coverage. However, its lower precision leads to a reduced F1 score (0.262). In contrast, <b>GPT-4o</b> attains the best overall F1 score (0.367), balancing recall and precision, while <b>InternVL-2.5</b> and <b>Qwen2.5-VL</b> achieve the highest precision (0.740 and 0.724 respectively), indicating conservative but accurate captioning.
          </p>
          <p>
            These results show that while some models excel at identifying more events (high recall), others favor accuracy (high precision). FIOVA-DQ highlights this trade-off and better reflects human alignment than lexical metrics.
          </p>
        </div>
        
        <div class="columns is-centered">
          <figure>
            <img src="static/images/t5.png" alt="Table 5">
            <!-- <figcaption style="text-align:center"><i>Table 5. Performance of 9 LVLMs on FIOVA<sub>hard</sub> (8-frame input).</i></figcaption> -->
          </figure>
        </div>
        
        <div class="content has-text-justified">
          <p>
            To explore temporal sensitivity, we further conduct frame-length ablation on selected models (Table 6). <b>GPT-4o</b> shows significant gains in both F1 (↑ from 0.367 to 0.418) and recall (↑ from 0.251 to 0.264) when increasing input length from 8 to 32 frames, confirming its ability to leverage temporal context. <b>InternVL-2.5</b> maintains high precision (↑ to 0.743), while <b>Tarsier</b> remains recall-oriented but plateaus in F1. These results indicate that longer temporal context can enhance performance for advanced models, while others may saturate.
          </p>
        </div>
        
        <div class="columns is-centered">
          <figure>
            <img src="static/images/t6.png" alt="Table 6">
            <!-- <figcaption style="text-align:center"><i>Table 6. Frame-length ablation results on FIOVA<sub>hard</sub>.</i></figcaption> -->
          </figure>
        </div>
        <br>
        
        <br />
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div style="text-align:center">
            <h2 class="title is-3">More Specific Examples</h2>
          </div>
          <br />
          
          <div class="content has-text-justified">
            <p>
              The following examples illustrate five common error types in LVLM-generated captions, each highlighted using a specific color:
            </p>
          
            <ul>
              <li>
                <span style="color:black;"><b>Omission</b></span>: Critical events or objects in the video are not described by the model.
                Although such omissions cannot be explicitly highlighted in the output, we provide textual analyses in the annotation to reveal these missing elements.
              </li>
              <li>
                <span style="color:purple;"><b>Misrepresentation</b></span>: The description includes content that is inconsistent with the video.
                These inaccuracies are <span style="color:purple;"><b>highlighted in purple</b></span>.
              </li>
              <li>
                <span style="color:rgb(255, 200, 0);"><b>Redundancy</b></span>: Repetitions of the same event or information within the caption.
                Such content is <span style="color:rgb(255, 200, 0);"><b>highlighted in yellow</b></span>.
              </li>
              <li>
                <span style="color:green;"><b>Excessive Extension</b></span>: The model introduces speculative or unnecessary details beyond what is present in the video.
                These are <span style="color:green;"><b>highlighted in green</b></span>.
              </li>
              <li>
                <span style="color:red;"><b>Hallucination</b></span>: The model generates content entirely absent from the video.
                These fabricated details are <span style="color:red;"><b>highlighted in red</b></span>.
              </li>
            </ul>
          
            <p>
              By categorizing and visualizing these errors, we provide a structured lens for analyzing the reliability of model-generated descriptions, and offer diagnostic insights into improving future LVLM performance.
            </p>
          </div>
          

          <div class="control has-icons-left">
            <div class="select is-medium is-info is-rounded ">
              <select id="dropdown" onchange="changeContent()" style="width:auto">
                <option value="example_1">Example 1</option>
                <option value="example_2">Example 2</option>
                <option value="example_3">Example 3</option>
                <option value="example_4">Example 4</option>
                <option value="example_5">Example 5</option>
                <option value="example_6">Example 6</option>
              </select>
              <div class="icon is-small is-left">
                <i class="fas fa-comment-alt"></i>
              </div>
            </div>
         </div>
          <!--/ Dropdown -->

          <!-- Content -->
          <div id="example_1" class="content-section">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/A19.png" alt="Figure A19">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">Human performance in video description tasks demonstrates remarkable consistency, especially in simpler scenarios where humans can effectively capture key content and provide accurate descriptions with minimal variation. In contrast, LVLMs exhibit significant limitations in these scenarios, often struggling to identify critical details and failing to match human descriptive ability. This discrepancy stems from the models' inability to fully comprehend the overall context and integrate video events with background information, which are essential for accurate and coherent descriptions. Among the LVLMs, models like LLaVA-NEXT-Video, Video-LLaVA, and VideoChat2 frequently exhibit issues of redundancy in their outputs. ShareGPT4Video shows pronounced hallucinations and repetitive descriptions, further highlighting its challenges in maintaining precision. Tarsier, while avoiding hallucination and excessive redundancy, suffers from omissions, such as neglecting the actions occurring after the boy lies on the ground. These findings underscore the persistent gap between LVLMs and human performance, particularly in scenarios requiring detailed understanding and contextual integration.</pre>
              </p>
            </div>
          </div>

          <div id="example_2" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/A20.png" alt="Figure A20">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">When video content is straightforward and visually intuitive, such as a person playing baseball or a noticeable change in scenery, there is minimal difference in performance between humans and LVLMs. Both can effectively capture key visual elements and generate accurate descriptions without relying on deep contextual or cultural understanding. This similarity highlights the ability of LVLMs to process intuitive visual information efficiently. However, in videos with frequent camera switches and more complex content, all models exhibit omissions in describing critical events. For example, ShareGPT4Video frequently includes redundant and repetitive descriptions, which detracts from its overall coherence. Furthermore, VideoChat2 demonstrates significant errors by misinterpreting the entire video as children playing, highlighting its limitations in accurately capturing contextual nuances and differentiating between events.</pre>
              </p>
            </div>
          </div>

          <div id="example_3" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/A21.png" alt="Figure A21">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">Significant semantic variation is evident in human descriptions of this video, underscoring the interpretative challenges posed by its strong artistic elements and intricate narrative structure. This variability stems from differing human perspectives, shaped by personal experiences, cultural backgrounds, and emotional interpretations, making it difficult to achieve consistent descriptive consensus. In contrast, LVLMs exhibit greater consistency in their descriptions but with notable shortcomings. The complexity of the video often overwhelms their ability to focus on specific scene details, leading to issues such as hallucinations, repetitive phrasing, and redundant content. These limitations are particularly evident in ShareGPT4Video, which frequently introduces irrelevant or speculative details, further reducing the coherence and accuracy of its descriptions. The contrast between human and LVLM performance reflects their fundamental differences. Humans naturally bring diverse, subjective viewpoints to complex video descriptions, which enriches their interpretations but decreases consistency. Meanwhile, LVLMs, trained on large datasets to identify and describe widely recognizable visual elements, prioritize consistency and universality over individuality. This standardization allows LVLMs to generate predictable outputs, though at the cost of nuanced understanding in highly artistic or subjective contexts.</pre>
              </p>
            </div>
          </div>

          <div id="example_4" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/A22.png" alt="Figure A22">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">The models demonstrated relatively poor performance in describing this video, primarily due to the intricate nature of its content. The video depicts a woman immersed in her fantasies, where the scenes transition frequently and contain temporal discontinuities. These features introduce substantial complexity, making it challenging for LVLMs to accurately interpret and coherently describe the video. The detailed elements within the fantasies, combined with the fragmented narrative structure, further exacerbate these challenges, leading to descriptions that are often unclear and lack interpretative depth. All LVLMs exhibited varying degrees of content omissions, failing to capture critical details of the video. Additionally, most models struggled with hallucinations and repetitive descriptions, further compromising the accuracy and coherence of their outputs. These limitations underscore the need for advancements in LVLMs to better handle videos with intricate, discontinuous narratives and rich contextual details.</pre>
              </p>
            </div>
          </div>


          <div id="example_5" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/A23.png" alt="Figure A23">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">The models demonstrated strong descriptive abilities for a video depicting Brazilian Jiu-Jitsu practice. The stable camera work and clear temporal relationships enabled accurate recognition and detailed description of activities and actions. LVLMs typically perform well on videos with simple, well-structured content, as such videos reduce interpretative ambiguity and allow the models to focus on key visual elements, showcasing their strengths in straightforward scenarios with minimal complexity. Despite the overall strong performance, some issues remain. Repetitive descriptions and redundancy were observed across the models, though hallucinations were relatively rare. Interestingly, while most LVLMs successfully identified the martial arts clothing worn by the characters, Video-LLaVA recognized only the color of the clothing without distinguishing its category. These results highlight the strengths and limitations of current LVLMs in processing videos with limited complexity.</pre>
              </p>
            </div>
          </div>

          <div id="example_6" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/A24.png" alt="Figure A24">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">Among the six LVLMs, ShareGPT4Video demonstrates pronounced hallucination issues, characterized by the generation of extensive redundant and irrelevant content. As shown in the accompanying figure, these shortcomings significantly detract from the model's descriptive precision and coherence, highlighting its challenges in effectively filtering and prioritizing relevant visual information.</pre>
              </p>
            </div>
          </div>



        </div>
      </div>
        

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
            <p>
              The templete of this webpage is based on <a href="https://llm-misinformation.github.io/">LLMs Meet Misinformation</a> project, thanks a lot for their good project webpage.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <div style="width: 250px; height: 150px; margin: 0 auto; overflow: hidden;">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=IP-BO8eFDnON9-3mPVwUX3KXPP3z9l5gAWRir4U2i7o&cl=ffffff&w=a"></script>
  </div>
  
</body>
<script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2", "example_3", "example_4", "example_5", "example_6"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>




</html>