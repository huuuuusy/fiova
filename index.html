
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="FIOVA (Five-in-One Video Annotations) Benchmark">
  <meta name="keywords" content="Video Caption, Video Understanding, LVLM Evaluation, Human-machine Comparison">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FIOVA (Five-in-One Video Annotations) Benchmark</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<meta property="og:title" content="FIOVA (Five-in-One Video Annotations) Benchmark" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://huuuuusy.github.io/fiova/" />
<meta property="og:url" content="https://huuuuusy.github.io/fiova/" />
<meta property="og:site_name" content="FIOVA (Five-in-One Video Annotations) Benchmark" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://huuuuusy.github.io/fiova/static/images/logo.png" />
<!-- <meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="FIOVA (Five-in-One Video Annotations) Benchmark" />
<meta name="twitter:description" content="FIOVA (Five-in-One Video Annotations) Benchmark." />
<meta name="twitter:site" content="@CanyuChen3" />
<meta name="twitter:image" content="https://huuuuusy.github.io/fiova/static/images/logo.png" /> -->
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"FIOVA (Five-in-One Video Annotations) Benchmark","name":"FIOVA (Five-in-One Video Annotations) Benchmark","url":"https://huuuuusy.github.io/fiova/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .grey-box {
        background-color: #c0c0c0; /* Grey color */
        color: rgb(70, 70, 70); /* White text color */
        padding: 20px; /* Padding inside the box */
        margin: 20px; /* Margin outside the box */
        text-align: center; /* Center the text */
    }
  </style>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">


</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/logo_1.png" alt="FIOVA (Five-in-One Video Annotations) Benchmark" width="900"/>
            <h1 class="title is-4 publication-title">This initiative aims to evaluate the gap between human and machine video understanding by benchmarking LVLMs against comprehensive human annotations.
            </h1>
            <h1 class="title is-5 publication-title">(Contact: <a href="https://huuuuusy.github.io/" target="_blank">Shiyu Hu</a>)
            </h1>

            <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
            <div class="content has-text-justified">
            <h3 class="head-h2" style="margin-bottom: 0.5em;color: rgb(255, 93, 93)">
              Latest News
            </h3>
                <!-- <li>
                    [2024.10.22] We have updated the <a href="https://arxiv.org/abs/2410.15270" target="_blank">arXiv version</a> of the paper.
                </li> -->
                <li>
                    [2025.05.15] The home page has been released! 
                </li>
            </div>
            </div>
            </div>
            </ul>
          </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="Can-LVLMs-Describe-Videos-Like-Humans" class="title is-2 publication-title">FIOVA: A Multi-Annotator Benchmark for Human-Aligned Video Captioning
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://huuuuusy.github.io/">Shiyu Hu*</a>,</span>
              <span class="author-block">
                <a href="https://xuchen-li.github.io/">Xuchen Li*</a>,</span>
              <span class="author-block">
                <a href="https://xuzhaoli.github.io/">Xuzhao Li</a>,</span>
              <span class="author-block">
                Jing Zhang,</span>
              <span class="author-block">
                <a href="https://github.com/updateforever">Yipei Wang</a>,</span>
              <span class="author-block">
                <a href="https://www.xinzhaoai.com/">Xin Zhao✉️</a>,</span>
              <span class="author-block">
                <a href="https://dr.ntu.edu.sg/cris/rp/rp02319">Kang Hao Cheong✉️</a></span>
            </div>

            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">Illinois Institute of Technology</span>
            </div> -->
            
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- <span class="link-block">
                  <a href="https://openreview.net/forum?id=ccxD4mtkTU" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Publication</span>
                  </a>
                </span> -->

                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.15270" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->

                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.15270" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/huuuuusy/FIOVA"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-archive"></i>
                    </span>
                    <span>Dataset</span>
                    </a>
                </span>

                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/live/4Tt4GYQ-ksk?si=JYX4PkBq8YzOLMG0&t=5243" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Talk</span>
                  </a>
                </span> -->

                <!-- <span class="link-block">
                  <a href="https://drive.google.com/file/d/12SbWsh6N_-a2-y-ZljDMCoMRq8z1yNO4/view?usp=sharing"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Slides 1</span>
                    </a>
                </span> -->

                <!-- <span class="link-block">
                  <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Slides 2</span>
                    </a>
                </span> -->
                
                <!-- <br>
                <span class="link-block">
                  <a href="https://zhuanlan.zhihu.com/p/678425256"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-zhihu"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span> -->

                <!-- <span class="link-block">
                  <a href="https://x.com/CanyuChen3/status/1749337997340790955?s=20"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-twitter"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span> -->

                <!-- <span class="link-block">
                  <a href="https://www.linkedin.com/posts/canyu-chen-1b2415100_iclr2024-misinformation-llm-activity-7155088736353972224--5Ng?utm_source=share&utm_medium=member_desktop"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-linkedin"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span> -->

                <!-- <span class="link-block">
                  <a href="https://github.com/llm-misinformation/llm-misinformation/tree/main/experiment/data"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                    </a>
                  </span> -->

              </div>
            </div>
            <!-- <div class="is-size-5 publication-authors">
              Published at <b><i>Proceedings of ICLR 2024</i></b>
            </div> -->
          </div>
        </div>
      </div>
    </div>


  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
              <figure>
                <img src="static/images/f1.png" style="width:75%" alt="Figure 0">
              </figure>
          </div>
        </div>
      </div> 
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite rapid progress in large vision-language models (LVLMs), existing video caption benchmarks remain limited in evaluating their alignment with human understanding. Most rely on a single annotation per video and lexical similarity-based metrics, failing to capture the variability in human perception and the cognitive importance of events. These limitations hinder accurate diagnosis of model capabilities in producing coherent, complete, and human-aligned descriptions.
              To address this, we introduce <strong>FIOVA</strong> (<strong>F</strong>ive-<strong>I</strong>n-<strong>O</strong>ne <strong>V</strong>ideo <strong>A</strong>nnotations), a human-centric benchmark tailored for evaluation. It comprises 3,002 real-world videos (~33.6s each), each annotated independently by five annotators. This design enables modeling of semantic diversity and inter-subjective agreement, offering a richer foundation for measuring human–machine alignment.
              We further propose <strong>FIOVA-DQ</strong>, an event-level evaluation metric that incorporates cognitive weights derived from annotator consensus, providing fine-grained assessment of event relevance and semantic coverage. Leveraging FIOVA, we conduct a comprehensive evaluation of nine representative LVLMs and introduce a complexity-aware analysis framework based on inter-annotator variation (CV). This reveals consistency gaps across difficulty levels and identifies structural issues such as event under-description and template convergence.
              Our results highlight FIOVA’s diagnostic value for understanding LVLM behavior under varying complexity, setting a new standard for cognitively aligned evaluation in long-video captioning. The benchmark, annotations, metric, and model outputs are publicly released to support future evaluation-driven research in video understanding.
            </p>
            
            </p>
          </div>
        </div>
      </div>
    </div>
  <!-- </section>
  <section class="section"> -->
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <!-- <br /> -->
      <div style="text-align:center">
        <h2 class="title is-3">Our Contributions</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>(1) <b>Human-centric dataset with multi-annotator diversity:</b> We introduce FIOVA, a benchmark comprising 3,002 real-world videos (average 33.6 seconds) covering 38 themes with complex spatiotemporal dynamics. Each video is annotated by five independent annotators, capturing diverse perspectives and establishing a cognitively grounded baseline for human-level video description. This design enables modeling of semantic variability and inter-subjective agreement, offering an authentic testbed for evaluating long video comprehension.</p>

        <p>(2) <b>Comprehensive evaluation of nine representative LVLMs:</b> We evaluate nine leading LVLMs, including GPT-4o, InternVL-2.5, and Qwen2.5-VL, across both full and high-complexity (FIOVA<em>hard</em>) subsets. Our tri-layer evaluation framework integrates traditional lexical metrics, event-level semantic alignment (AutoDQ), and cognitively weighted evaluation (FIOVA-DQ), revealing trade-offs between precision and recall as well as consistent model weaknesses under narrative ambiguity.</p>
        
        <p>(3) <b>Complexity-aware human–machine comparison with cognitive alignment:</b> We propose a novel analysis framework based on inter-annotator variation (CV) to categorize videos by difficulty, enabling batch-level diagnosis of model robustness and behavior patterns. Our findings show that while humans increase descriptive diversity under complexity, LVLMs often converge to rigid, template-like outputs. FIOVA-DQ effectively captures this divergence by emphasizing event importance and aligning better with human judgment.</p>
        
        <br>

        <br>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
            <p>
              The templete of this webpage is based on <a href="https://llm-misinformation.github.io/">LLMs Meet Misinformation</a> project, thanks a lot for their good project webpage.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=IP-BO8eFDnON9-3mPVwUX3KXPP3z9l5gAWRir4U2i7o&cl=ffffff&w=a"></script>
</body>
<script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2", "example_3", "example_4", "example_5", "example_6"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script>




</html>